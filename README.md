
# ğŸ§  Knapsack GFlowNet  
_Efficient, probabilistic 0â€‘1 Knapsack optimisation for Economists, OR folks and ML Engineers_

---

## ğŸš€ Oneâ€‘Minute Pitch

The classic **0â€‘1 Knapsack** problem asks:  
*Which items do I pick to maximize total value without exceeding my budget?*

Every choice is discreteâ€”you either take an item (`1`) or skip it (`0`).  
This simple rule creates sequences like `0 1 0 1 â€¦` across multiple items,  
leading to **2â¿ possible combinations**â€”an exponential complexity nightmare.

Rather than brute-forcing every possibility,  
a **Generative Flow Network (GFlowNet)** learns a **probability distribution**  
over the entire space of combinations, guiding you probabilistically toward the best solutions.

### ğŸ”§ How does it work?

The GFlowNet looks at four simple things at each decision step:

1. **Item utilities** *(how valuable is each item?)*  
2. **Item prices** *(what does each item cost?)*  
3. **Remaining budget** *(how much money is left?)*  
4. **Items already picked or skipped**

It outputs exactly **one number**â€”a probability between 0 and 1.  
You take this probability and **flip a loaded coin**: pick or skip the item, building up a decision sequence step by step.  
You repeat this process **128 times in parallel**, so the model quickly learns where the best solutions lie.

### ğŸ§  Where does Z come in?

Each sequence you create has a **reward** (e.g., total value in utility). But rewards are arbitrary numbers, not probabilitiesâ€”they donâ€™t sum to 1 naturally.

To fix this, we introduce **Z**, a learnable parameter:

$$P(\text{sequence}) = \frac{\text{reward(sequence)}}{Z}$$

**Z** converts raw rewards into a proper probability distribution.  
It acts like a **currency converter**â€”turning arbitrary reward values into probabilities.  
The model continuously adjusts **Z** during training to ensure that probabilities match their rewards proportionally.  

Put simply:  
- High-reward solutions â†’ higher probabilities  
- Lower-reward solutions â†’ lower probabilities  
- Total probability always sums neatly to 1, thanks to Z

With this smart, self-balancing system,  
for *n = 15* items, the global optimum usually emerges after about **5,120 samples**, instead of naively checking **all 32,768 possibilities**.

---

### ğŸ’¥ Summary

The GFlowNet doesnâ€™t just search blindly â€”  
it **learns to guide probability flow** toward high-value solutions,  
turning a combinatorial explosion into an efficient, scalable process.

---

## ğŸ‘¥ Audience Cheatâ€‘Sheet

| You areâ€¦ | Read **first** | Why this repo is useful |
|----------|---------------|-------------------------|
| **ğŸ“ Economist** (no heavy mathsÂ /Â AI) | `0_A-ECON.md` | The model learns to guide sampling toward the best solutions, without needing to explore everything exhaustively |
| **ğŸ›  OR / Optimisation** | `0_B-OR.md` | Think *Monteâ€‘Carlo DP Ã— Normalising Flows* â€“ sample instead of enumerate |
| **ğŸ¤– ML / AI Engineer** | `0_C-ML.md` | Trajectoryâ€‘Balance loss, GPU batching, W&B sweeps, three state encodings |

> Each primer is two paragraphs; skip what you already know.

---

## ğŸ—‚ï¸ Repository Layout

```
.
â”œâ”€â”€ assets/                    # Images for README (graphs, results)
â”‚   â”œâ”€â”€ Multi.png              # Hyperparameter sweep & CDF plots
â”‚   â””â”€â”€ ...                    # (other visuals if needed)
â”œâ”€â”€ data/                      # Pickled toy instance (u, t, B)
â”‚   â””â”€â”€ data.pickle
â”œâ”€â”€ env/
â”‚   â””â”€â”€ environment.yaml       # Conda definition (Python 3.13.2)
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ probability_distribution_on_batch.py
â”œâ”€â”€ models/                    # Three GFlowNet variants
â”‚   â”œâ”€â”€ baseline_v1.py         # v1 â€“ budget-blind
â”‚   â”œâ”€â”€ block_traj_v2.py       # v2 â€“ online blocking
â”‚   â””â”€â”€ remaining_budget_v3.py # v3 â€“ dynamic budget âœ… best
â”œâ”€â”€ reward/
â”‚   â”œâ”€â”€ analytical_reward.py   # Brute-force oracle (â‰¤20 items)
â”‚   â””â”€â”€ reward.py              # Batch reward computation
â”œâ”€â”€ sweep/
â”‚   â””â”€â”€ sweep.yaml             # W&B Bayesian optimisation config
â”œâ”€â”€ train.py                   # ğŸ Entry-point
â”œâ”€â”€ requirements.txt           # pip alternative to Conda
â””â”€â”€ README.md                  # â† you are here
```

*(A local `wandb/` directory is created at runâ€‘time; add it to `.gitignore`.)*

---

## ğŸ”¬ Algorithm in a Nutshell
1. **State** = `(-1, 0, 1)` code for each item + **remaining budget** (v3).  
2. **Policy** `Ï€_Î¸(aÂ |Â s)` queried for 128 states **in parallel**.  
3. **Collapsed Trajectory-Balance**  

   $$
   \mathcal{L}(\tau) = \left( \log P_\theta(\tau) + \log Z_\theta - \log R(\tau) \right)^2
   $$

   with a learnable scalar `logÂ Z`.  
4. Gradient step â†’ discard batch â†’ sample a fresh one. Eventually every leaf is visited stochastically.

---

## ğŸ“ˆ Key Results

### ğŸ“„ Full Report

For a detailed analysis of the experiments, including additional figures and discussion:
ğŸ‘‰ **[Read the full report here](https://api.wandb.ai/links/arthurmaffre-alone/u6krh0mc)**

### ğŸ“Š Hyperparameter Sweep Results

![Hyperparameter and Reward Analysis](assets/Multi.png)

The following figure shows:
- The **parallel coordinates** of hyperparameter influence on KL divergence.
- **Max reward** evolution across different runs.
- **CDF alignment** between sampled and analytical distributions.

_Interactive dashboards on WeightsÂ &Â Biases project **gflownetâ€‘knapsack**._

---

## âš¡ Quickâ€‘Start

### 1Â Â·Â Environment

```bash
git clone https://github.com/yourâ€‘handle/gflownetâ€‘knapsack.git
cd gflownetâ€‘knapsack

conda env create -f env/environment.yaml
conda activate gflownet-knapsack
# orÂ Â python -m pip install -r requirements.txt
```

### 2Â Â·Â Train

```bash
python train.py --model_version v3 --num_epochs 1000 --batch_size 128
```

Dataset default: `data/data.pickle` â€“ override with `--data_path`.

### 3Â Â·Â Hyperâ€‘parameter sweep (optional)

```bash
wandb sweep sweep/sweep.yaml
wandb agent <entity>/<project>/<sweep-id>
```

Objective = minimise **mean_kl_200_to_end**.

---

## ğŸ’¾ Data Format

```python
{
  "u": torch.FloatTensor(n),  # utilities
  "t": torch.FloatTensor(n),  # costs
  "B": float                  # budget
}
```

---

## ğŸ›  Extensions
* **Bilevel optimisation** â€“ reuse across similar instances.  
* **Transformer encoder** â€“ scale beyond 25 items.  
* **Distributed training** â€“ multiâ€‘GPUÂ /Â multiâ€‘node.

PRs welcome!

---

## ğŸ“œ Licence
MIT

> _Made with â¤ï¸ & a MacBook Air (M2) chilled on an iceâ€‘pack._
