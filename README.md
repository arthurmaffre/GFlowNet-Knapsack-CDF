
# ğŸ§  Knapsack GFlowNet  
_Efficient, probabilistic 0â€‘1 Knapsack optimisation for Economists, OR folks and ML Engineers_

---

## ğŸš€ Oneâ€‘Minute Pitch
The classic **0â€‘1 Knapsack** has 2^n feasible baskets.  
A **Generative Flow Network (GFlowNet)** learns a *probability distribution* over **all** baskets so that higherâ€‘value sets are sampled more often.  
For *n = 15* the global optimum is typically found after **â‰ˆ 2â€¯600** sampled trajectories (20 epochs Ã— 128â€‘batch) instead of traversing all 32â€¯768 leaves.

---

## ğŸ‘¥ Audience Cheatâ€‘Sheet

| You areâ€¦ | Read **first** | Why this repo is useful |
|----------|---------------|-------------------------|
| **ğŸ“ Economist** (no heavy mathsÂ /Â AI) | `0_A-ECON.md` | Study the *whole* welfare distribution, not a single optimum |
| **ğŸ›  OR / Optimisation** | `0_B-OR.md` | Think *Monteâ€‘Carlo DP Ã— Normalising Flows* â€“ sample instead of enumerate |
| **ğŸ¤– ML / AI Engineer** | `0_C-ML.md` | Trajectoryâ€‘Balance loss, GPU batching, W&B sweeps, three state encodings |

> Each primer is two paragraphs; skip what you already know.

---

## ğŸ—‚ï¸ Repository Layout

```
.
â”œâ”€â”€ data/                       # Pickled toy instance  (u, t, B)
â”‚   â””â”€â”€ data.pickle
â”œâ”€â”€ env/
â”‚   â””â”€â”€ environment.yaml        # Conda definition  (PythonÂ 3.13.2)
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ probability_distribution_on_batch.py
â”œâ”€â”€ models/                     # Three GFlowNet variants
â”‚   â”œâ”€â”€ baseline_v1.py          # v1 â€“ budgetâ€‘blind
â”‚   â”œâ”€â”€ block_traj_v2.py        # v2 â€“ online blocking
â”‚   â””â”€â”€ remaining_budget_v3.py  # v3 â€“ dynamic budget  âœ… best
â”œâ”€â”€ reward/
â”‚   â”œâ”€â”€ analytical_reward.py    # Bruteâ€‘force oracle (â‰¤20 items)
â”‚   â””â”€â”€ reward.py               # Batch reward computation
â”œâ”€â”€ sweep/
â”‚   â””â”€â”€ sweep.yaml              # W&B Bayesian optimisation config
â”œâ”€â”€ train.py                    # ğŸ Entryâ€‘point
â”œâ”€â”€ requirements.txt            # pip alternative to Conda
â””â”€â”€ README.md                   # â† you are here
```

*(A local `wandb/` directory is created at runâ€‘time; add it to `.gitignore`.)*

---

## ğŸ”¬ Algorithm in a Nutshell
1. **State** = `(-1, 0, 1)` code for each item + **remaining budget** (v3).  
2. **Policy** `Ï€_Î¸(aÂ |Â s)` queried for 128 states **in parallel**.  
3. **Trajectoryâ€‘Balance loss**  

   \[
   \mathcal L(Ï„)=\bigl[\log P_{Î¸}(Ï„)+\log Z-\log R(Ï„)\bigr]^2
   \]

   with a learnable scalar `logÂ Z`.  
4. Gradient step â†’ discard batch â†’ sample a fresh one. Eventually every leaf is visited stochastically.

---

## ğŸ“ˆ Key Results



_Interactive dashboards on WeightsÂ &Â Biases project **gflownetâ€‘knapsack**._

---

## âš¡ Quickâ€‘Start

### 1Â Â·Â Environment

```bash
git clone https://github.com/yourâ€‘handle/gflownetâ€‘knapsack.git
cd gflownetâ€‘knapsack

conda env create -f env/environment.yaml
conda activate gflownet-knapsack
# orÂ Â python -m pip install -r requirements.txt
```

### 2Â Â·Â Train

```bash
python train.py --model_version v3 --num_epochs 1000 --batch_size 128
```

Dataset default: `data/data.pickle` â€“ override with `--data_path`.

### 3Â Â·Â Hyperâ€‘parameter sweep (optional)

```bash
wandb sweep sweep/sweep.yaml
wandb agent <entity>/<project>/<sweep-id>
```

Objective = minimise **mean_kl_200_to_end**.

---

## ğŸ’¾ Data Format

```python
{
  "u": torch.FloatTensor(n),  # utilities
  "t": torch.FloatTensor(n),  # costs
  "B": float                  # budget
}
```

---

## ğŸ›  Extensions
* **Bilevel optimisation** â€“ reuse across similar instances.  
* **Transformer encoder** â€“ scale beyond 25 items.  
* **Distributed training** â€“ multiâ€‘GPUÂ /Â multiâ€‘node.

PRs welcome!

---

## ğŸ“œ Licence
MIT

> _Made with â¤ï¸ & a MacBook Air (M2) chilled on an iceâ€‘pack._
