"""
train.py â€“ Knapsack GFlowNet Trainer
====================================

ğŸš€ ONE-MINUTE PITCH â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Solve the 0-1 knapsack problem **without exhaustively enumerating** all 2â¿
solutions.  A Generative Flow Network (GFlowNet) learns a *probability
distribution* over **every feasible item-set**, so high-value combinations are
sampled often while low-value ones fade away.  In practice we reach the global
optimum in â‰ˆ 2 600 sampled trajectories (20 epochs Ã— 128-batch) instead of
exploring all 32 768 paths.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0. Audience Guide
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ ğŸ“ **Economists (no heavy maths / AI)**  
  â€“ Think of this as a *smarter survey* of every shopping basket under a
    budget-cap; the network learns which baskets give the best bang-for-buck
    and how likely each is.  
  â€“ Output = a *distribution*, not just â€œone bestâ€ bundle, so you can study
    alternative allocations and their welfare.

â€¢ ğŸ›  **Operations-Research / Optimisation folks (no econ / AI)**  
  â€“ GFlowNet â‰ˆ â€œMonte-Carlo Dynamic Programming Ã— normalised flowsâ€.  
  â€“ It *implicitly* explores the binary decision tree **without breadth-first
    DP**; sample efficiency is O(10Â²â€“10Â³) vs exhaustive 2â¿.  
  â€“ Provides *provably unbiased* estimates of the optimal value via
    Trajectory-Balance; our runs hit the true optimum on *every* seed so far.

â€¢ ğŸ¤– **AI / ML Engineers (no econ / maths)**  
  â€“ Standard TB loss:  `log PÎ¸(Ï„) + log Z = log R(Ï„)` with learnable `log Z`.  
  â€“ Three state-representations (v1 baseline, v2 block-traj, v3 dynamic-budget).  
  â€“ Integrated Weights & Biases logging, GPU/MPS auto-detect, Sweep-API for
    Bayesian hyper-search.  All plots in the screenshots (parallel coords,
    CDFs, KL curves) are logged out-of-the-box.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ‘  Visual intuition
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1)  The search tree (3 items = 2Â³ paths) â€“ ASCII snapshot
    â”Œâ”€â”€â”€(root)
    â”‚
    â”œâ”€â”€0â”€â”€â”€0â”€â”€â”€0   â† skip, skip, skip
    â”‚   â”‚   â””â”€â”€1   â† skip, skip, **take**
    â”‚   â””â”€â”€1â”€â”€â”€0
    â”‚       â””â”€â”€1
    â””â”€â”€1â”€â”€â”€0â”€â”€â”€0   â† **take**, â€¦
        â”‚   â””â”€â”€1
        â””â”€â”€1â”€â”€â”€0
            â””â”€â”€1   â† â€¦take, take, take (leaf)

    â€¢ Inside the model we encode the embedding vector (or context vector for the model) 
        them with -1, 0, 1 : skip, not seen wet, take
    â€¢ DP would *visit* every leaf.  
    â€¢ GFlowNet *samples* leaves â€“ high-reward ones (bold) appear with
      higher probability but low-reward paths are still occasionally drawn,
      so we stay exploratory.

2)  How a GFlowNet rolls out a trajectory â€“ Mermaid sequence
    ```mermaid
    sequenceDiagram
        participant s0 as state sâ‚€ (budget B)
        participant Ï€ as Ï€Î¸ (GFlowNet)
        participant s1 as state sâ‚
        participant s2 as state sâ‚‚
        participant End as terminal Ï„

        s0 ->> Ï€ : context = [sâ‚€]
        Ï€  -->> s0 : P(aâ‚€=take/skip)
        Note over s0,Ï€: GPU â‡’ 128 states queried **in parallel**

        Ï€ ->> s1 : sample aâ‚€, update<br/>remaining budget
        s1 ->> Ï€ : context = [sâ‚]
        Ï€  -->> s1 : P(aâ‚)
        Ï€ ->> s2 : â€¦

        loop until all items seen
            â€¦
        end
        sâ‚‚ ->> End : trajectory Ï„, reward R(Ï„)
    ```
    â€¢ All 128 trajectories in a batch share the same forward pass, so the GPU
      builds 128 â€œbranchesâ€ at once instead of one after the other.  
    â€¢ **Batching** = massive speed-up versus naÃ¯ve tree search.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        Note over s0,Ï€:
          On every forward pass we **sample** 128 partial states in parallel.  
          âœ The policy Ï€Î¸ is *global*: its logits are defined for **all** 2â¿
            possible prefixes; we just train it with *mini-batches*.  
          After the gradient step we throw these 128 paths away and draw a new
          batch, so over many epochs the whole tree is covered stochastically.


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Problem Statement â€“ 0-1 Knapsack (NP-hard)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Given *utilities* uáµ¢, *costs* táµ¢, and a budget B, choose a binary vector
x âˆˆ {0,1}â¿ that maximises âˆ‘uáµ¢xáµ¢ subject to âˆ‘táµ¢xáµ¢ â‰¤ B.

A traditional DP traverses the full binary tree (2â¿ leaves).  
With n = 15 thatâ€™s 32 768 nodes; with n = 50 itâ€™s 1 Peta-paths.  
**GFlowNet** learns to *sample* from that tree so paths with high reward
appear often, yet exploration never collapses too early.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2. Why GFlowNet instead of Classic RL or DP?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
| Approach | What it does | Limitation | How GFlowNet fixes it |
|----------|--------------|------------|-----------------------|
| **DP / Tree-search** | Visits *every* path | Exponential blow-up | Samples O(10Â³) paths instead |
| **RL (e.g. Q-learning)** | Optimises one trajectory | Can get stuck in a local optimum | Learns a **distribution** â‡’ diversity + exploration |
| **GFlowNet** | Optimises *probability* of each path s.t. P âˆ reward | â€” | Finds optimum & its neighbourhood efficiently |

Empirically on 15 items we converge in â‰ˆ 2 600 sampled trajectories.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3. Model Variants
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
v1 baseline        â€“ Ignores budget during generation; infeasible baskets get
                     reward = 10 (small but non-zero to avoid log(0)).  
v2 block_traj      â€“ *Online* feasibility check: any choice that would overspend
                     is forced to 0, trajectory continues.  
v3 dynamic_budget  â€“ Augments the state with â€œremaining budgetâ€, letting the
                     policy spend intelligently. **Best performer.**

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4. Loss Function â€“ Trajectory Balance
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For every sampled sequence Ï„:

    L(Ï„) = (log PÎ¸(Ï„) + log Z  â€“  log R(Ï„))Â²

`log Z` is a single scalar parameter; learning it stabilises gradients.  
KL divergence to the ideal distribution is logged each epoch.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
5. WandB Instrumentation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Logs: loss, KL, `log Z`, mean / max reward, CDF plots every N epochs.  
â€¢ Sweeps: define `sweep.yaml` then run `wandb agent <entity>/<proj>/<sweep-id>`.  
  Bayesian optimisation over `lr_main`, `lr_z`, `embedding_dim`, etc.  
â€¢ Screenshots above show:  
  â€“ *Parameter importance* (feature bars)  
  â€“ *Parallel-coordinates* of hyperparameters vs performance  
  â€“ *CDF alignment* (target vs sampled distribution)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
6. Quick-Start
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

python train.py

or 

python train.py --arg=value ... for running it with args

"""


from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Tuple, Dict

import torch
import wandb
from tqdm import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Local imports â€“ keep them grouped for clarity
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from reward.reward import compute_reward
from reward.analytical_reward import compute_analytical_reward
from metrics.probability_distribution_on_batch import log_distribution_and_cdf

from models.baseline_v1 import GFlowNet as BaselineV1
from models.block_traj_v2 import GFlowNet as BlockTrajV2
from models.remaining_budget_v3 import GFlowNet as DynamicBudgetV3

# Mapping from CLI flag to concrete class
MODEL_MAP = {
    "v1": BaselineV1,
    "v2": BlockTrajV2,
    "v3": DynamicBudgetV3,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def select_device() -> torch.device:
    """Return the *best* available torch device (CUDAÂ >Â MPSÂ >Â CPU)."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def prepare_tensors(data: Dict[str, torch.Tensor], batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:
    """Expand 1â€‘D data arrays to batched tensors on the chosen *device*."""
    u = torch.tensor(data["u"], dtype=torch.float32, device=device).expand(batch_size, -1)
    t = torch.tensor(data["t"], dtype=torch.float32, device=device).expand(batch_size, -1)
    B = torch.tensor(data["B"], dtype=torch.float32, device=device).view(1, 1).expand(batch_size, 1)
    return u.detach(), t.detach(), B.detach(), u.size(1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Training loop
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def train(cfg: argparse.Namespace) -> None:  # noqa: C901 (keep flat for readability)
    """Endâ€‘toâ€‘end training routine driven by a parsed *cfg* namespace."""
    # 1Â â€“Â WeightsÂ &Â Biases initialisation -------------------------------------------------
    run_name = f"{cfg.model_version}_bs{cfg.batch_size}_ep{cfg.num_epochs}_{wandb.util.generate_id()[:5]}"
    wandb.init(project="gflownet-knapsack", name=run_name, config=vars(cfg))

    # 2Â â€“Â Device & data -------------------------------------------------------------------
    device = select_device()
    print(f"ğŸ“Ÿ Â Training on {device.type.upper()}")

    with Path(cfg.data_path).open("rb") as fh:
        raw_data = pickle.load(fh)

    max_reward_bruteforce = compute_analytical_reward(
        u_vals=raw_data["u"], t_vals=raw_data["t"], B_val=raw_data["B"]
    )
    wandb.config.update({"max_reward_bruteforce": max_reward_bruteforce})

    u, t, B, num_items = prepare_tensors(raw_data, cfg.batch_size, device)

    # 3Â â€“Â Model & optimiser ---------------------------------------------------------------
    ModelCls = MODEL_MAP[cfg.model_version]
    model = ModelCls(
        num_items=num_items,
        embedding_dim=cfg.embedding_dim,
        hidden_dim=cfg.hidden_dim,
        init_value_z=cfg.init_value_z,
    ).to(device)

    log_z_params = [model.log_z]
    other_params = [p for p in model.parameters() if p is not model.log_z]

    optimizer = torch.optim.SGD(
        [
            {"params": other_params, "lr": cfg.lr_main, "momentum": cfg.mom_main, "weight_decay": 1e-4},
            {"params": log_z_params, "lr": cfg.lr_z, "momentum": cfg.mom_z, "weight_decay": 0.0},  # no decay on *z*
        ]
    )

    # 4Â â€“Â Training loop -------------------------------------------------------------------
    best_reward = 0.0
    kl_history = []  # KL values after epochÂ 200 (for sweep metric)

    pbar = tqdm(range(cfg.num_epochs), desc=f"ModelÂ {cfg.model_version} | {device.type.upper()}")
    for epoch in pbar:
        optimizer.zero_grad()

        seq_logp, selected = model.generate_trajectories(B, u, t, cfg.batch_size, num_items, device)
        assert seq_logp.requires_grad, "sequence_log_prob should be differentiable!"

        reward = compute_reward(selected, u, t, B, num_items)
        loss = model.compute_loss(seq_logp, reward)
        loss.backward()
        optimizer.step()

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Evaluation (noâ€‘grad) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with torch.no_grad():
            avg_reward = reward.mean().item()
            max_reward = reward.max().item()
            best_reward = max(best_reward, max_reward)

            prob_seq = torch.exp(seq_logp)
            p_target = reward / reward.sum()
            prob_seq = prob_seq / prob_seq.sum()
            kl = (p_target * (torch.log(p_target + 1e-6) - torch.log(prob_seq + 1e-6))).sum().item()

            if epoch >= 200:
                kl_history.append(kl)

            # Optional diagnostic CDF plot (every 50 epochs)
            log_distribution_and_cdf(epoch, reward, seq_logp, p_target, wandb, step_interval=50)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Progress bar & W&B logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        pbar.set_postfix(Loss=f"{loss.item():.4f}", Rew_Avg=f"{avg_reward:.2f}", Rew_Max=f"{max_reward:.2f}", KL=f"{kl:.4f}")

        wandb.log({
            "loss": loss.item(),
            "avg_reward": avg_reward,
            "kl_div": kl,
            "max_reward": best_reward,
            "z": model.log_z.item(),
        }, step=epoch)

    # 5Â â€“Â Postâ€‘training summary -----------------------------------------------------------
    if kl_history:
        mean_kl = sum(kl_history) / len(kl_history)
        if not mean_kl:  # handle exact zero or NaN
            wandb.log({"mean_kl_200_to_end": 2.0, "kl_zero": True})
        else:
            wandb.log({"mean_kl_200_to_end": mean_kl})
            print(f"Mean KL (200â€‘{cfg.num_epochs}) = {mean_kl:.4f}")

    wandb.finish()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI entryâ€‘point
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train GFlowNet knapsack with W&B + SGD")

    # Core hyperâ€‘parameters -------------------------------------------------------------
    parser.add_argument("--model_version", choices=list(MODEL_MAP), default="v1", help="Model variant to train (v1 or v2 or v3)")
    parser.add_argument("--data_path", type=str, default="data.pickle", help="Pickle file with u, t, B")
    parser.add_argument("--batch_size", type=int, default=128, help="Batch size")
    parser.add_argument("--num_epochs", type=int, default=400, help="Number of training epochs")

    parser.add_argument("--embedding_dim", type=int, default=95)
    parser.add_argument("--hidden_dim", type=int, default=230)
    parser.add_argument("--init_value_z", type=float, default=9.5)

    # Optimiser hyperâ€‘parameters --------------------------------------------------------
    parser.add_argument("--lr_main", type=float, default=2e-3, help="Learning rate for main params")
    parser.add_argument("--lr_z", type=float, default=4e-4, help="Learning rate for z param")
    parser.add_argument("--mom_main", type=float, default=0.9, help="Momentum for main params")
    parser.add_argument("--mom_z", type=float, default=0.9, help="Momentum for z param")

    cfg = parser.parse_args()
    train(cfg)
